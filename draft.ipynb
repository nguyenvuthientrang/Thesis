{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from dataloaders.utils import *\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader,Subset\n",
    "import yaml\n",
    "from dataloaders import utils\n",
    "from dataloaders.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from dataloaders.utils import *\n",
    "from dataloaders.utils import check_integrity\n",
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader,Subset\n",
    "import yaml\n",
    "\n",
    "\n",
    "class iDatasetBA(data.Dataset):\n",
    "    \n",
    "    def __init__(self, root,\n",
    "                train=True, transform=None,\n",
    "                download_flag=False, lab=True, swap_dset = None, \n",
    "                tasks=None, seed=-1, rand_split=False, validation=False, kfolds=5,\n",
    "                backdoor=True, target=1, indices=None, noise=None, attack_transform=None):\n",
    "\n",
    "        # process rest of args\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.validation = validation\n",
    "        self.seed = seed\n",
    "        self.t = -1\n",
    "        self.tasks = tasks\n",
    "        self.download_flag = download_flag\n",
    "        self.backdoor = backdoor\n",
    "        self.indices = indices\n",
    "        self.noise = noise\n",
    "        self.target = target\n",
    "        self.attack_transform = attack_transform\n",
    "\n",
    "        # load dataset\n",
    "        self.load()\n",
    "        self.num_classes = len(np.unique(self.targets))\n",
    "\n",
    "        # remap labels to match task order\n",
    "        c = 0\n",
    "        self.class_mapping = {}\n",
    "        self.class_mapping[-1] = -1\n",
    "        for task in self.tasks:\n",
    "            for k in task:\n",
    "                self.class_mapping[k] = c\n",
    "                c += 1\n",
    "\n",
    "        # targets as numpy.array\n",
    "        self.data = np.asarray(self.data)\n",
    "        self.targets = np.asarray(self.targets)\n",
    "\n",
    "        self.archive = []\n",
    "        domain_i = 0\n",
    "        for task in self.tasks:\n",
    "            if True:\n",
    "                locs = np.isin(self.targets, task).nonzero()[0]\n",
    "                self.archive.append((self.data[locs].copy(), self.targets[locs].copy()))\n",
    "\n",
    "        if self.train:\n",
    "            self.coreset = (np.zeros(0, dtype=self.data.dtype), np.zeros(0, dtype=self.targets.dtype))\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index, simple = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.indices is not None:\n",
    "            if index in self.indices:\n",
    "                img = self.attack_transform[0](img)\n",
    "                img = torch.clamp(apply_noise_patch(self.noise,img,mode='add'),-1,1)\n",
    "                img = self.attack_transform[1](img)\n",
    "            else:\n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.class_mapping[target], self.t\n",
    "\n",
    "\n",
    "\n",
    "    def load_dataset(self, t, train=True):\n",
    "        \n",
    "        if train:\n",
    "            self.data, self.targets = self.archive[t] \n",
    "        else:\n",
    "            self.data    = np.concatenate([self.archive[s][0] for s in range(t+1)], axis=0)\n",
    "            self.targets = np.concatenate([self.archive[s][1] for s in range(t+1)], axis=0)\n",
    "        self.t = t\n",
    "        if self.target in self.tasks[t]:\n",
    "            self.update_indices()\n",
    "        else:\n",
    "            self.indices = None\n",
    "    \n",
    "    def update_indices(self):\n",
    "        train_label = self.targets\n",
    "        train_target_list = list(np.where(np.array(train_label)==self.target)[0])\n",
    "        random_poison_idx = random.sample(train_target_list, poison_amount)\n",
    "\n",
    "        self.indices = random_poison_idx\n",
    "\n",
    "    def append_coreset(self, only=False, interp=False):\n",
    "        len_core = len(self.coreset[0])\n",
    "        if self.train and (len_core > 0):\n",
    "            if only:\n",
    "                self.data, self.targets = self.coreset\n",
    "            else:\n",
    "                len_data = len(self.data)\n",
    "                sample_ind = np.random.choice(len_core, len_data)\n",
    "                self.data = np.concatenate([self.data, self.coreset[0][sample_ind]], axis=0)\n",
    "                self.targets = np.concatenate([self.targets, self.coreset[1][sample_ind]], axis=0)\n",
    "\n",
    "    def update_coreset(self, coreset_size, seen):\n",
    "        num_data_per = coreset_size // len(seen)\n",
    "        remainder = coreset_size % len(seen)\n",
    "        data = []\n",
    "        targets = []\n",
    "        \n",
    "        # random coreset management; latest classes take memory remainder\n",
    "        # coreset selection without affecting RNG state\n",
    "        state = np.random.get_state()\n",
    "        np.random.seed(self.seed)\n",
    "        for k in reversed(seen):\n",
    "            mapped_targets = [self.class_mapping[self.targets[i]] for i in range(len(self.targets))]\n",
    "            locs = (mapped_targets == k).nonzero()[0]\n",
    "            if (remainder > 0) and (len(locs) > num_data_per):\n",
    "                num_data_k = num_data_per + 1\n",
    "                remainder -= 1\n",
    "            else:\n",
    "                num_data_k = min(len(locs), num_data_per)\n",
    "            locs_chosen = locs[np.random.choice(len(locs), num_data_k, replace=False)]\n",
    "            data.append([self.data[loc] for loc in locs_chosen])\n",
    "            targets.append([self.targets[loc] for loc in locs_chosen])\n",
    "        self.coreset = (np.concatenate(list(reversed(data)), axis=0), np.concatenate(list(reversed(targets)), axis=0))\n",
    "        np.random.set_state(state)\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    def apply_noise_patch(noise,images,offset_x=0,offset_y=0,mode='change',padding=20,position='fixed'):\n",
    "        '''\n",
    "        noise: torch.Tensor(1, 3, pat_size, pat_size)\n",
    "        images: torch.Tensor(N, 3, 512, 512)\n",
    "        outputs: torch.Tensor(N, 3, 512, 512)\n",
    "        '''\n",
    "        length = images.shape[2] - noise.shape[2]\n",
    "        if position == 'fixed':\n",
    "            wl = offset_x\n",
    "            ht = offset_y\n",
    "        else:\n",
    "            wl = np.random.randint(padding,length-padding)\n",
    "            ht = np.random.randint(padding,length-padding)\n",
    "        if images.dim() == 3:\n",
    "            noise_now = noise.clone()[0,:,:,:]\n",
    "            wr = length-wl\n",
    "            hb = length-ht\n",
    "            m = nn.ZeroPad2d((wl, wr, ht, hb))\n",
    "            if(mode == 'change'):\n",
    "                images[:,ht:ht+noise.shape[2],wl:wl+noise.shape[3]] = 0\n",
    "                images += m(noise_now)\n",
    "            else:\n",
    "                images += noise_now\n",
    "        else:\n",
    "            for i in range(images.shape[0]):\n",
    "                noise_now = noise.clone()\n",
    "                wr = length-wl\n",
    "                hb = length-ht\n",
    "                m = nn.ZeroPad2d((wl, wr, ht, hb))\n",
    "                if(mode == 'change'):\n",
    "                    images[i:i+1,:,ht:ht+noise.shape[2],wl:wl+noise.shape[3]] = 0\n",
    "                    images[i:i+1] += m(noise_now)\n",
    "                else:\n",
    "                    images[i:i+1] += noise_now\n",
    "        return images\n",
    "\n",
    "\n",
    "\n",
    "class iCIFAR10BA(iDatasetBA):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the iDataset Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "    im_size=32\n",
    "    nch=3\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        # download dataset\n",
    "        if self.download_flag:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train or self.validation:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.course_targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "                if 'coarse_labels' in entry:\n",
    "                    self.course_targets.extend(entry['coarse_labels'])\n",
    "                \n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        self._load_meta()\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "class iCIFAR100BA(iCIFAR10BA):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the iCIFAR10 Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }\n",
    "    im_size=32\n",
    "    nch=3\n",
    "\n",
    "class iDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, root,\n",
    "                train=True, transform=None,\n",
    "                download_flag=False, lab=True, swap_dset = None, \n",
    "                tasks=None, seed=-1, rand_split=False, validation=False, kfolds=5):\n",
    "\n",
    "        # process rest of args\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.validation = validation\n",
    "        self.seed = seed\n",
    "        self.t = -1\n",
    "        self.tasks = tasks\n",
    "        self.download_flag = download_flag\n",
    "\n",
    "        # load dataset\n",
    "        self.load()\n",
    "        self.num_classes = len(np.unique(self.targets))\n",
    "\n",
    "        # remap labels to match task order\n",
    "        c = 0\n",
    "        self.class_mapping = {}\n",
    "        self.class_mapping[-1] = -1\n",
    "        for task in self.tasks:\n",
    "            for k in task:\n",
    "                self.class_mapping[k] = c\n",
    "                c += 1\n",
    "\n",
    "        # targets as numpy.array\n",
    "        self.data = np.asarray(self.data)\n",
    "        self.targets = np.asarray(self.targets)\n",
    "\n",
    "        # if validation\n",
    "        if self.validation:\n",
    "            \n",
    "            # shuffle\n",
    "            state = np.random.get_state()\n",
    "            np.random.seed(self.seed)\n",
    "            randomize = np.random.permutation(len(self.targets))\n",
    "            self.data = self.data[randomize]\n",
    "            self.targets = self.targets[randomize]\n",
    "            np.random.set_state(state)\n",
    "\n",
    "            # sample\n",
    "            n_data = len(self.targets)\n",
    "            if self.train:\n",
    "                self.data = self.data[:int(0.8*n_data)]\n",
    "                self.targets = self.targets[:int(0.8*n_data)]\n",
    "            else:\n",
    "                self.data = self.data[int(0.8*n_data):]\n",
    "                self.targets = self.targets[int(0.8*n_data):]\n",
    "\n",
    "            # train set\n",
    "            if self.train:\n",
    "                self.data = self.data[:int(0.8*n_data)]\n",
    "                self.targets = self.targets[:int(0.8*n_data)]\n",
    "                self.archive = []\n",
    "                domain_i = 0\n",
    "                for task in self.tasks:\n",
    "                    if True:\n",
    "                        locs = np.isin(self.targets, task).nonzero()[0]\n",
    "                        self.archive.append((self.data[locs].copy(), self.targets[locs].copy()))\n",
    "\n",
    "            # val set\n",
    "            else:\n",
    "                self.archive = []\n",
    "                domain_i = 0\n",
    "                for task in self.tasks:\n",
    "                    if True:\n",
    "                        locs = np.isin(self.targets, task).nonzero()[0]\n",
    "                        self.archive.append((self.data[locs].copy(), self.targets[locs].copy()))\n",
    "\n",
    "        # else\n",
    "        else:\n",
    "            self.archive = []\n",
    "            domain_i = 0\n",
    "            for task in self.tasks:\n",
    "                if True:\n",
    "                    locs = np.isin(self.targets, task).nonzero()[0]\n",
    "                    self.archive.append((self.data[locs].copy(), self.targets[locs].copy()))\n",
    "\n",
    "        if self.train:\n",
    "            self.coreset = (np.zeros(0, dtype=self.data.dtype), np.zeros(0, dtype=self.targets.dtype))\n",
    "\n",
    "    def __getitem__(self, index, simple = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class\n",
    "        \"\"\"\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.class_mapping[target], self.t\n",
    "\n",
    "    def load_dataset(self, t, train=True):\n",
    "        \n",
    "        if train:\n",
    "            self.data, self.targets = self.archive[t] \n",
    "        else:\n",
    "            self.data    = np.concatenate([self.archive[s][0] for s in range(t+1)], axis=0)\n",
    "            self.targets = np.concatenate([self.archive[s][1] for s in range(t+1)], axis=0)\n",
    "        self.t = t\n",
    "\n",
    "    def append_coreset(self, only=False, interp=False):\n",
    "        len_core = len(self.coreset[0])\n",
    "        if self.train and (len_core > 0):\n",
    "            if only:\n",
    "                self.data, self.targets = self.coreset\n",
    "            else:\n",
    "                len_data = len(self.data)\n",
    "                sample_ind = np.random.choice(len_core, len_data)\n",
    "                self.data = np.concatenate([self.data, self.coreset[0][sample_ind]], axis=0)\n",
    "                self.targets = np.concatenate([self.targets, self.coreset[1][sample_ind]], axis=0)\n",
    "\n",
    "    def update_coreset(self, coreset_size, seen):\n",
    "        num_data_per = coreset_size // len(seen)\n",
    "        remainder = coreset_size % len(seen)\n",
    "        data = []\n",
    "        targets = []\n",
    "        \n",
    "        # random coreset management; latest classes take memory remainder\n",
    "        # coreset selection without affecting RNG state\n",
    "        state = np.random.get_state()\n",
    "        np.random.seed(self.seed)\n",
    "        for k in reversed(seen):\n",
    "            mapped_targets = [self.class_mapping[self.targets[i]] for i in range(len(self.targets))]\n",
    "            locs = (mapped_targets == k).nonzero()[0]\n",
    "            if (remainder > 0) and (len(locs) > num_data_per):\n",
    "                num_data_k = num_data_per + 1\n",
    "                remainder -= 1\n",
    "            else:\n",
    "                num_data_k = min(len(locs), num_data_per)\n",
    "            locs_chosen = locs[np.random.choice(len(locs), num_data_k, replace=False)]\n",
    "            data.append([self.data[loc] for loc in locs_chosen])\n",
    "            targets.append([self.targets[loc] for loc in locs_chosen])\n",
    "        self.coreset = (np.concatenate(list(reversed(data)), axis=0), np.concatenate(list(reversed(targets)), axis=0))\n",
    "        np.random.set_state(state)\n",
    "\n",
    "    def load(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "class iCIFAR10(iDataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the iDataset Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'batches.meta',\n",
    "        'key': 'label_names',\n",
    "        'md5': '5ff9c542aee3614f3951f8cda6e48888',\n",
    "    }\n",
    "    im_size=32\n",
    "    nch=3\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        # download dataset\n",
    "        if self.download_flag:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train or self.validation:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.course_targets = []\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        for file_name, checksum in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(f)\n",
    "                else:\n",
    "                    entry = pickle.load(f, encoding='latin1')\n",
    "                self.data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.targets.extend(entry['labels'])\n",
    "                else:\n",
    "                    self.targets.extend(entry['fine_labels'])\n",
    "                if 'coarse_labels' in entry:\n",
    "                    self.course_targets.extend(entry['coarse_labels'])\n",
    "                \n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        self._load_meta()\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta['filename'])\n",
    "        if not check_integrity(path, self.meta['md5']):\n",
    "            raise RuntimeError('Dataset metadata file not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        with open(path, 'rb') as infile:\n",
    "            if sys.version_info[0] == 2:\n",
    "                data = pickle.load(infile)\n",
    "            else:\n",
    "                data = pickle.load(infile, encoding='latin1')\n",
    "            self.classes = data[self.meta['key']]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "class iCIFAR100(iCIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the iCIFAR10 Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n",
    "    meta = {\n",
    "        'filename': 'meta',\n",
    "        'key': 'fine_label_names',\n",
    "        'md5': '7973b15100ade9c7d40fb424638fde48',\n",
    "    }\n",
    "    im_size=32\n",
    "    nch=3\n",
    "\n",
    "class iIMAGENET_R(iDataset):\n",
    "    \n",
    "    base_folder = 'imagenet-r'\n",
    "    im_size=224\n",
    "    nch=3\n",
    "    def load(self):\n",
    "\n",
    "        # load splits from config file\n",
    "        if self.train or self.validation:\n",
    "            data_config = yaml.load(open('dataloaders/splits/imagenet-r_train.yaml', 'r'), Loader=yaml.Loader)\n",
    "        else:\n",
    "            data_config = yaml.load(open('dataloaders/splits/imagenet-r_test.yaml', 'r'), Loader=yaml.Loader)\n",
    "        self.data = data_config['data']\n",
    "        self.targets = data_config['targets']\n",
    "\n",
    "    def __getitem__(self, index, simple = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class\n",
    "        \"\"\"\n",
    "        img_path, target = self.data[index], self.targets[index]\n",
    "        img = jpg_image_to_array(img_path)\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.class_mapping[target], self.t\n",
    "\n",
    "    def parse_archives(self) -> None:\n",
    "        if not check_integrity(os.path.join(self.root, META_FILE)):\n",
    "            parse_devkit_archive(self.root)\n",
    "\n",
    "        if not os.path.isdir(self.split_folder):\n",
    "            if self.split == 'train':\n",
    "                parse_train_archive(self.root)\n",
    "            elif self.split == 'val':\n",
    "                parse_val_archive(self.root)\n",
    "\n",
    "    @property\n",
    "    def split_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.split)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"Split: {split}\".format(**self.__dict__)\n",
    "\n",
    "def jpg_image_to_array(image_path):\n",
    "    \"\"\"\n",
    "    Loads JPEG image into 3D Numpy array of shape \n",
    "    (width, height, channels)\n",
    "    \"\"\"\n",
    "    with Image.open(image_path) as image:      \n",
    "        image = image.convert('RGB')\n",
    "        im_arr = np.fromstring(image.tobytes(), dtype=np.uint8)\n",
    "        im_arr = im_arr.reshape((image.size[1], image.size[0], 3))                                   \n",
    "    return \n",
    "\n",
    "def get_datasets(args, trainDataset, outterDataset, tasks, resize_imnet, seed, phase='trigger_gen'):\n",
    "    if phase == 'trigger_gen':\n",
    "        train_transform = utils.get_transform(dataset=args.dataset, phase='train', aug=args.train_aug, resize_imnet=resize_imnet)\n",
    "        ori_train = trainDataset(args.dataroot, train=True, lab = True, tasks=tasks,\n",
    "                            download_flag=True, transform=train_transform, \n",
    "                            seed=seed, rand_split=args.rand_split, validation=args.validation)\n",
    "        outter = outterDataset(args.dataroot, train=True, lab = True, tasks=tasks,\n",
    "                            download_flag=True, transform=train_transform, \n",
    "                            seed=seed, rand_split=args.rand_split, validation=args.validation)\n",
    "        train_label = [get_labels(ori_train)[x] for x in range(len(get_labels(ori_train)))]\n",
    "        train_target_list = list(np.where(np.array(train_label)==args.target_lab)[0])\n",
    "        train_target = Subset(ori_train,train_target_list)\n",
    "        return outter, train_target\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# train_transform = utils.get_transform(dataset=\"CIFAR100\", phase='train', aug=True, resize_imnet=True)\n",
    "tasks = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
    "seed = 0\n",
    "target_lab = 50\n",
    "train_transform = utils.get_transform(dataset=\"CIFAR100\", phase='train', aug=True, resize_imnet=True)\n",
    "ori_train = iCIFAR100('data', train=True, lab = True, tasks=tasks,\n",
    "                    download_flag=True, transform=train_transform, \n",
    "                    seed=seed, rand_split=True, validation=False)\n",
    "outter = iCIFAR100('data', train=True, lab = True, tasks=tasks,\n",
    "                    download_flag=True, transform=train_transform, \n",
    "                    seed=seed, rand_split=True, validation=False)\n",
    "train_label = [get_labels(ori_train)[x] for x in range(len(get_labels(ori_train)))]\n",
    "train_target_list = list(np.where(np.array(train_label)==target_lab)[0])\n",
    "train_target = Subset(ori_train,train_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_target\u001b[39m.\u001b[39;49mload_dataset(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataset.py:83\u001b[0m, in \u001b[0;36mDataset.__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m function\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
